[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Bibliography & Other Resources",
    "section": "",
    "text": "Mandatory Readings\nRother, Lynn, Max Koss, and Fabio Mariani. â€œTaking Care of History: Toward a Politics of Provenance Linked Open Data in Museums.â€ In Perspectives on Data, edited by Emily Lew Fry and Erin Canning. Art Institute of Chicago, 2022.\nhttps://www.artic.edu/digital-publications/37/perspectives-on-data/25/taking-care-of-history\nSchich, Maximilian, Christian Huemer, Patryk Adamczyk, Lev Manovich, and Yang-Yu Liu. â€œNetwork Dimensions in the Getty Provenance Index.â€ arXiv (2017). https://arxiv.org/abs/1706.02804\n\n\n\nGetty Provenance Index Tutorials\nIntroduction to the Getty Provenance Index\nArtists, Collectors & Dealers in the GPI\nUsing the Getty Provenance Index Databases\nGetty Provenance Index Explained: A Walkthrough\n\n\n\nBibliography\nAhnert, Ruth, and Sebastian E. Ahnert. Tudor Networks of Power. Oxford University Press, 2023.\nhttps://doi.org/10.1093/oso/9780198858973.001.0001\nAhnert, Ruth, et al. The Network Turn. Cambridge University Press, 2020.\nhttps://doi.org/10.1017/9781108866804\nBarabÃ¡si, Albert-LÃ¡szlÃ³, et al. Network Science.\nhttp://networksciencebook.com/\nBishop, Claire. â€œAgainst Digital Art History.â€ International Journal for Digital Art History 3 (2018).\nhttps://doi.org/10.11588/dah.2018.3.49915\nCairo, Alberto. How Charts Lie: Getting Smarter About Visual Information. W. W. Norton, 2020.\nDâ€™Ignazio, Catherine, and Lauren F. Klein. Data Feminism. MIT Press, 2023.\nhttps://data-feminism.mitpress.mit.edu/\nDrucker, Johanna. Graphesis: Visual Forms of Knowledge Production. Harvard University Press, 2014.\nDrucker, Johanna. â€œIs There a â€˜Digitalâ€™ Art History?â€ Visual Resources 29, no. 1â€“2 (2013): 5â€“13.\nhttps://doi.org/10.1080/01973762.2013.761106\nFraiberger, Samuel P. et al. â€œQuantifying Reputation and Success in Art.â€ Science 362, no. 6416 (2018): 825â€“829.\nhttps://doi.org/10.1126/science.aau7224\nHuemer, Christian. â€œThe Provenance of Provenances.â€ In Collecting & Provenance: A Multidisciplinary Approach, edited by Jane C. Milosch and Nick Pearce. Rowman & Littlefield, 2019.\nKarsdorp, Folgert. Python Programming for the Humanities.\nhttps://www.karsdorp.io/python-course/\nKarsdorp, Folgert, Mike Kestemont, and Allen Riddell. Humanities Data Analysis: Case Studies with Python. Princeton University Press, 2021.\nhttps://www.humanitiesdataanalysis.org/\nKlein, Lauren F. â€œThe Image of Absence: Archival Silence, Data Visualization, and James Hemings.â€ American Literature 85, no. 4 (2013): 661â€“688.\nhttps://doi.org/10.1215/00029831-2367310\nLincoln, Matthew D. â€œTangled Metaphors: Network Thinking and Network Analysis in the History of Art.â€ In The Routledge Companion to Digital Humanities and Art History. Routledge, 2020.\nhttps://doi.org/10.4324/9780429505188\nLove, Heather. â€œClose but Not Deep: Literary Ethics and the Descriptive Turn.â€ New Literary History 41, no. 2 (2010): 371â€“391.\nhttp://www.jstor.org/stable/40983827\nMariani, Fabio. â€œIntroducing VISU: Vagueness, Incompleteness, Subjectivity, and Uncertainty in Art Provenance Data.â€ In Computational Methods in the Humanities 2022, CEUR Workshop Proceedings 3602 (2023).\nhttps://ceur-ws.org/Vol-3602/\nMariani, Fabio, Max Koss, and Lynn Rother. â€œPeople Information in Provenance Data: Biographical Entity Linking with Wikidata and ULAN.â€ Å½ivot Umjetnosti 114, no. 1 (2024): 148â€“161.\nhttps://doi.org/10.31664/zu.2024.114.07\nMariani, Fabio, Lynn Rother, and Max Koss. â€œTeaching Provenance to AI: An Annotation Scheme for Museum Data.â€ In AI in Museums: Reflections, Perspectives and Applications, edited by Sonja Thiel and Johannes Bernhardt. Transcript Verlag, 2023.\nhttps://doi.org/10.1515/9783839467107-014\nMcNeill, John R., and William H. McNeill. The Human Web: A Birdâ€™s-Eye View of World History. W. W. Norton, 2003.\nMeirelles, Isabel. Design for Information. Rockport Publishers, 2013.\nPoirier, Lindsay. â€œEthnographies of Datasets: Teaching Critical Data Analysis Through R Notebooks.â€ 2020.\nhttps://lindsaypoirier.github.io/publication/2020-12-01-Ethnographies-of-Datasets-Teaching-Critical-Data-Analysis-through-R-Notebooks\nPorras, Stephanie. â€œKeeping Our Eyes Open: Visualizing Networks and Art History.â€ Artl@s Bulletin 6, no. 3 (2017).\nhttps://docs.lib.purdue.edu/artlas/vol6/iss3/3\nRicaurte, Paola. â€œData Epistemologies, the Coloniality of Power, and Resistance.â€ Television & New Media 20, no. 4 (2019): 350â€“365.\nhttps://doi.org/10.1177/1527476419831640\nRother, Lynn. â€œUncanny Provenance: Art History and Its Double / Unheimliche Provenienz: Kunstgeschichte und ihr DoppelgÃ¤nger.â€ Texte zur Kunst 32, no. 128 (2022): 84â€“97.\nRother, Lynn. â€œInstitutionalisierte Institutionskritik: Lynn Rother Ã¼ber â€˜Wege der Kunstâ€™ im Museum Rietberg, ZÃ¼rich, sowie â€˜Zerrissene Moderneâ€™ und â€˜Der Sammler Curt Glaserâ€™ im Kunstmuseum Basel.â€ Texte zur Kunst 33, no. 131 (2023): 214â€“219.\nRother, Lynn, Max Koss, and Fabio Mariani. â€œInterpreting Strings, Weaving Threads: Structuring Provenance Data with AI.â€ In Sammlungsforschung im Digitalen Zeitalter, edited by Stefan Alschner and Katharina GÃ¼nther. Wallstein Verlag, 2024.\nhttps://doi.org/10.15499/kds-005-008\nSedig, Kamran, and Paul Parsons. Design of Visualizations for Humanâ€“Information Interaction: A Pattern-Based Framework. Springer Nature, 2016.\nhttps://doi.org/10.1007/978-3-031-02602-7\nShekhtman, Louis Michael, and Albert-LÃ¡szlÃ³ BarabÃ¡si. â€œPhilanthropy in Art: Locality, Donor Retention, and Prestige.â€ Scientific Reports 13, no. 1 (2023): 12157.\nhttps://doi.org/10.1038/s41598-023-38815-1\nTuscher, Michaela, et al.Â  â€œNodes, Edges, and Artistic Wedges: A Survey on Network Visualization in Art History.â€ Computer Graphics Forum (2025): e70154.\nhttps://doi.org/10.1111/cgf.70154\n\n\n\nOther Resources\nCARE Principles for Indigenous Data Governance\nCIDOC CRM, Version 7.3\nCosmograph\nCreating a Data Cleaning Workflow\nDataviz Project\nGephi\nGetty Vocabularies (AAT, ULAN, TGN)\nIllustrated Guide to Transformer: A component-by-component breakdown analysis (2020)\niAnalyzer\nLinked Art â€” Linked Art Community\nOpenRefine\nProgramming Historian\nPython Graph Gallery\nRAWGraphs\nText Analysis for Humanities Research (with a focus on Archaeology)\nThe FAIR Guiding Principles for Scientific Data Management and Stewardship\nThe Illustrated Word2vec\nThe Virtual International Authority File (VIAF)\nVisualizing Historical Networks\nVoyant Tools\nWikidata: A Free and Open Knowledge Base\n\n\n\nÂ© 2025 Coding Provenance Workshop â€” Licensed under a Creative Commons Attribution 4.0 International License.\nReuse is encouraged â€” we happily promote changes of custody. Just be sure to write a good provenance record and cite this workshop."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Our Mission and Vision & Organizers",
    "section": "",
    "text": "The workshop\nThis workshop is a collaboration between the Provenance Lab and the Getty Provenance Index.\nAfter decades working as provenance and art history researchers, we have witnessed the field changing and evolving in many different ways. We believe that the intersection between computational techniques and provenance studies can not only reveal patterns, structures, and dynamics that were hard to uncover through close reading, but also lead us to new research questions that were previously impossible to answer or to know.\nOur aim with this workshop is to equip researchers and practitioners with concrete skills and to introduce them to methodologies that can inspire new policies, practices, and analytical approaches within their institutions or individual research projects. We believe that interdisciplinary collaboration, between art historians, provenance researchers, museum professionals, computer scientists, and digital humanities specialists, is essential. However, for these collaborations to be fair, productive, and balanced, all participants need a shared understanding of what is possible, how different methods work, and what their limitations are.\nThis workshop is designed as a first encounter with key theories, methods, techniques, and tools. Our goal is to open a conversation, spark critical thinking, and lay the groundwork for a collaborative effort that will continue to grow beyond these sessions.\n\n\n\nOrganizers and Instructors\n\n\n\n\nBÃ¡rbara Romero FerrÃ³n\n\n\nProvenance Lab, Leuphana University of LÃ¼neburg\n\n\n\n\n\nLynn Rother\n\n\nProvenance Lab, Leuphana University of LÃ¼neburg\n\n\n\n\n\nGiulia Taurino\n\n\nGetty Provenance Index, Getty Research Institute\n\n\n\n\n\nSandra van Ginhoven\n\n\nGetty Provenance Index, Getty Research Institute\n\n\n\n\n\n\n\nOther Instructors & Organization Team\n\n\n\n\nMax Koss\n\n\nProvenance Lab, Leuphana University of LÃ¼neburg\n\n\n\n\n\nFabio Mariani\n\n\nUniversity of Augsburg\n\n\n\n\n\n\n\nCoco Rufer\n\n\nProvenance Lab, Leuphana University of LÃ¼neburg\n\n\n\n\n\nKatharina Hilgert\n\n\nProvenance Lab, Leuphana University of LÃ¼neburg\n\n\n\n\n\n\nJulia Knapmeyer\n\n\nProvenance Lab, Leuphana University of LÃ¼neburg\n\n\n\n\n\nÂ© 2025 Coding Provenance Workshop â€” Licensed under a Creative Commons Attribution 4.0 International License.\nReuse is encouraged â€” we happily promote changes of custody. Just be sure to write a good provenance record and cite this workshop."
  },
  {
    "objectID": "nlp.html",
    "href": "nlp.html",
    "title": "From Raw Text to Structured Data",
    "section": "",
    "text": "Schema\n\nIntro to Natural Language Processing (NLP)\n\nIntro to Large Language Models (LLMs)\n\n\n\nContent Summary\nFrom Raw Text to Structured Data: A Briefing on NLP and LLMs for Provenance Studies\n\n\nSlides\n\n\n\n\n\n\nActivity\nDownload this provenance record sample: The Eternal Feminine (Lâ€™Ã‰ternel FÃ©minin), Paul Cezanne, c.1877, Provenance Records\n\nPython Scripts\nA comprehensive walkthrough of NLP preprocessing and analysis tasks using art provenance data as our example text.\n\n\n\n\n\n\nRequired installations\n\n\n\n\n\n!pip install nltk spacy\n!python -m spacy download en_core_web_sm\n!python -m spacy download en_core_web_md  # For word vectors\n\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom collections import Counter\nimport spacy\n\n# Download required NLTK data (run once)\ndef setup_nltk():\n    \"\"\"Download required NLTK resources.\"\"\"\n    resources = ['punkt', 'punkt_tab', 'averaged_perceptron_tagger',\n                 'averaged_perceptron_tagger_eng', 'wordnet', 'stopwords']\n    for resource in resources:\n        try:\n            nltk.download(resource, quiet=True)\n        except:\n            pass\n\nsetup_nltk()\n\n\n\n\n\n\n\n\n\nOpen Txt\n\n\n\n\n\nwith open('/content/eternal_femenine_provenance_text.txt', 'r') as f:\n    PROVENANCE_TEXT = f.read()\nprint(PROVENANCE_TEXT)\n\n\n\n\n\n\n\n\n\nSentece Segmentation\n\n\n\n\n\ndef exercise_sentence_segmentation(text):\n    \"\"\"\n    EXERCISE 1: Sentence Segmentation\n    ----------------------------------\n    Split text into individual sentences.\n\n    Challenge: Provenance text has unusual punctuation patterns\n    (dates, abbreviations) that can confuse simple segmenters.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"STEP 1: SENTENCE SEGMENTATION\")\n    print(\"=\" * 80)\n\n    # Method 1: NLTK sent_tokenize\n    sentences_nltk = sent_tokenize(text)\n\n    print(f\"\\nğŸ“Š Found {len(sentences_nltk)} sentences using NLTK:\\n\")\n    for i, sent in enumerate(sentences_nltk, 1):\n        print(f\"  Sentence {i}: {sent.strip()[:80]}...\")\n        if len(sent.strip()) &gt; 80:\n            print(f\"              {sent.strip()[80:]}\")\n        print()\n\n    # Method 2: spaCy (often better for complex text)\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(text)\n    sentences_spacy = list(doc.sents)\n\n    print(f\"\\nğŸ“Š Found {len(sentences_spacy)} sentences using spaCy:\\n\")\n    for i, sent in enumerate(sentences_spacy, 1):\n        print(f\"  Sentence {i}: {str(sent).strip()[:80]}...\")\n        print()\n\n    return sentences_nltk\n\nsentences = exercise_sentence_segmentation(PROVENANCE_TEXT)\n\n\n\n\n\n\n\n\n\nN-Gram Generation\n\n\n\n\n\ndef exercise_ngram_generation(tokens):\n    \"\"\"\n    EXERCISE 3: N-Gram Generation\n    -----------------------------\n    Generate sequences of N consecutive tokens.\n\n    Why important: Captures multi-word entities like \"Wildenstein & Co.\"\n    and \"Jean-Victor Pellerin\"\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"STEP 3: N-GRAM GENERATION\")\n    print(\"=\" * 80)\n\n    # Define tokens\n    tokens = word_tokenize(PROVENANCE_TEXT)\n\n    # Filter out punctuation for cleaner n-grams\n    clean_tokens = [t for t in tokens if t.isalnum()]\n\n    # Generate different n-grams\n    unigrams = list(ngrams(clean_tokens, 1))\n    bigrams = list(ngrams(clean_tokens, 2))\n    trigrams = list(ngrams(clean_tokens, 3))\n\n    print(f\"\\nğŸ“Š Unigrams (1-grams): {len(unigrams)} total\")\n    print(f\"  Sample: {unigrams[:5]}\")\n\n    print(f\"\\nğŸ“Š Bigrams (2-grams): {len(bigrams)} total\")\n    print(f\"  Sample: {bigrams[:10]}\")\n\n    print(f\"\\nğŸ“Š Trigrams (3-grams): {len(trigrams)} total\")\n    print(f\"  Sample: {trigrams[:10]}\")\n\n    # Most common n-grams (useful for finding patterns)\n    print(\"\\nğŸ“ˆ Most Common Bigrams:\")\n    bigram_freq = Counter(bigrams)\n    for bg, count in bigram_freq.most_common(10):\n        print(f\"  {' '.join(bg)}: {count}\")\n\n    print(\"\\nğŸ“ˆ Most Common Trigrams:\")\n    trigram_freq = Counter(trigrams)\n    for tg, count in trigram_freq.most_common(10):\n        print(f\"  {' '.join(tg)}: {count}\")\n\n    # Identify potential named entities from n-grams\n    print(\"\\nğŸ” Potential Multi-Word Entities (from bigrams):\")\n    name_patterns = [bg for bg in bigrams if bg[0][0].isupper() and bg[1][0].isupper()]\n    for pattern in list(set(name_patterns))[:10]: # Fixed: Convert set to list for slicing\n        print(f\"  - {' '.join(pattern)}\")\n\n    return bigrams, trigrams\n\nbigrams, trigrams = exercise_ngram_generation(tokens)\n\n\n\n\n\n\n\n\n\nStemming and Lemmmatization\n\n\n\n\n\ndef exercise_stemming_lemmatization(tokens):\n    \"\"\"\n    EXERCISE 4: Stemming and Lemmatization\n    --------------------------------------\n    Reduce words to their root/base form.\n\n    Stemming: Rule-based chopping (fast but crude)\n    Lemmatization: Dictionary-based (accurate but slower)\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"STEP 4: STEMMING AND LEMMATIZATION\")\n    print(\"=\" * 80)\n\n    stemmer = PorterStemmer()\n    lemmatizer = WordNetLemmatizer()\n\n    # Filter to meaningful words\n    words = [t.lower() for t in tokens if t.isalpha() and len(t) &gt; 2]\n    unique_words = list(set(words))[:20]\n\n    print(\"\\nğŸ“Š Comparison: Original vs Stemmed vs Lemmatized\\n\")\n    print(f\"  {'Original':&lt;20} {'Stemmed':&lt;20} {'Lemmatized':&lt;20}\")\n    print(f\"  {'-'*20} {'-'*20} {'-'*20}\")\n\n    for word in unique_words:\n        stemmed = stemmer.stem(word)\n        lemmatized = lemmatizer.lemmatize(word, pos='v')  # Treat as verb\n\n        # Highlight differences\n        stem_marker = \"â†\" if stemmed != word else \"\"\n        lemma_marker = \"â†\" if lemmatized != word else \"\"\n\n        print(f\"  {word:&lt;20} {stemmed:&lt;20} {lemmatized:&lt;20} {stem_marker}{lemma_marker}\")\n\n    # Special focus on verbs in our text\n    print(\"\\nğŸ” Verb Analysis (important for provenance):\")\n    verbs = ['sold', 'founded', 'dissolved', 'inheritance']\n    print(f\"\\n  {'Original':&lt;15} {'Stemmed':&lt;15} {'Lemmatized':&lt;15}\")\n    print(f\"  {'-'*15} {'-'*15} {'-'*15}\")\n    for verb in verbs:\n        print(f\"  {verb:&lt;15} {stemmer.stem(verb):&lt;15} {lemmatizer.lemmatize(verb, pos='v'):&lt;15}\")\n\n    return unique_words\n\nstemmed_words = exercise_stemming_lemmatization(tokens)\n\n\n\n\n\n\n\n\n\nStop Word Analysis\n\n\n\n\n\ndef exercise_stop_word_analysis(tokens):\n    \"\"\"\n    EXERCISE 5: Stop Word Analysis\n    ------------------------------\n    Identify and optionally remove common words with little semantic value.\n\n    Caution: In provenance text, some \"stop words\" like \"to\" and \"by\"\n    are actually meaningful (e.g., \"sold to\", \"by inheritance\")\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"STEP 5: STOP WORD ANALYSIS\")\n    print(\"=\" * 80)\n\n    stop_words = set(stopwords.words('english'))\n\n    # Separate tokens\n    word_tokens = [t.lower() for t in tokens if t.isalpha()]\n    stopped = [t for t in word_tokens if t in stop_words]\n    content_words = [t for t in word_tokens if t not in stop_words]\n\n    print(f\"\\nğŸ“Š Stop Word Statistics:\")\n    print(f\"  - Total word tokens: {len(word_tokens)}\")\n    print(f\"  - Stop words found: {len(stopped)} ({len(stopped)/len(word_tokens)*100:.1f}%)\")\n    print(f\"  - Content words: {len(content_words)} ({len(content_words)/len(word_tokens)*100:.1f}%)\")\n\n    print(f\"\\nğŸ›‘ Stop Words Found:\")\n    print(f\"  {Counter(stopped).most_common(15)}\")\n\n    print(f\"\\nâœ… Content Words (after stop word removal):\")\n    print(f\"  {Counter(content_words).most_common(20)}\")\n\n    # Domain-specific consideration\n    print(\"\\nâš ï¸  IMPORTANT: Context-Sensitive Stop Words\")\n    print(\"  In provenance text, these 'stop words' carry meaning:\")\n    print(\"  - 'to' â†’ indicates transfer recipient ('sold to')\")\n    print(\"  - 'by' â†’ indicates method ('by inheritance')\")\n    print(\"  - Consider keeping these for downstream tasks!\")\n\n    # Custom stop word list for provenance\n    custom_keep = {'to', 'by', 'from'}\n    provenance_content = [t for t in word_tokens\n                          if t not in stop_words or t in custom_keep]\n\n    print(f\"\\nğŸ“Š With Custom Rules (keeping 'to', 'by', 'from'):\")\n    print(f\"  Content words: {len(provenance_content)}\")\n\n    return content_words\n\ncontent_words = exercise_stop_word_analysis(tokens)\n\n\n\n\n\n\n\n\n\nWord Embeddings\n\n\n\n\n\ndef exercise_word_embeddings(text):\n    \"\"\"\n    EXERCISE 6: Word Embeddings\n    ---------------------------\n    Convert words to dense vector representations that capture semantic meaning.\n\n    Using spaCy's pre-trained word vectors (en_core_web_md has 300-dim vectors)\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"STEP 6: WORD EMBEDDINGS\")\n    print(\"=\" * 80)\n\n    # Try to load medium model with word vectors\n    try:\n        nlp = spacy.load(\"en_core_web_md\")\n        has_vectors = True\n    except OSError:\n        print(\"\\nâš ï¸  en_core_web_md not found. Using en_core_web_sm (limited vectors)\")\n        print(\"   Install with: python -m spacy download en_core_web_md\")\n        nlp = spacy.load(\"en_core_web_sm\")\n        has_vectors = False\n\n    doc = nlp(text)\n\n    # Get unique meaningful words\n    words_to_embed = ['sold', 'purchased', 'bought', 'inheritance',\n                      'gallery', 'museum', 'collector', 'dealer',\n                      'Paris', 'York', 'France', 'American']\n\n    print(\"\\nğŸ“Š Word Vector Examples:\")\n    print(f\"  Vector dimensions: {nlp.vocab['sold'].vector.shape[0]}\")\n\n    if has_vectors:\n        print(\"\\n  Sample vector for 'sold' (first 10 dimensions):\")\n        sold_vector = nlp.vocab['sold'].vector[:10]\n        print(f\"  {sold_vector}\")\n\n        # Semantic similarity\n        print(\"\\nğŸ“Š Semantic Similarity (cosine similarity):\")\n        word_pairs = [\n            ('sold', 'purchased'),\n            ('sold', 'bought'),\n            ('sold', 'museum'),\n            ('gallery', 'museum'),\n            ('Paris', 'France'),\n            ('Paris', 'museum'),\n            ('collector', 'dealer'),\n        ]\n\n        for w1, w2 in word_pairs:\n            token1 = nlp(w1)\n            token2 = nlp(w2)\n            similarity = token1.similarity(token2)\n            bar = \"â–ˆ\" * int(similarity * 20)\n            print(f\"  {w1:&lt;12} â†” {w2:&lt;12}: {similarity:.3f} {bar}\")\n\n        # Find similar words\n        print(\"\\nğŸ” Words Most Similar to 'sold':\")\n        # This is a simplified approach - real word2vec would use gensim\n        test_words = ['bought', 'purchased', 'traded', 'gave', 'inherited',\n                      'museum', 'gallery', 'collector']\n        sold_token = nlp('sold')\n        similarities = [(w, sold_token.similarity(nlp(w))) for w in test_words]\n        for word, sim in sorted(similarities, key=lambda x: x[1], reverse=True):\n            bar = \"â–ˆ\" * int(sim * 20)\n            print(f\"    {word:&lt;15}: {sim:.3f} {bar}\")\n    else:\n        print(\"\\n  (Limited vector support with small model)\")\n\n    return doc\n\ndoc = exercise_word_embeddings(PROVENANCE_TEXT)\n\n\n\n\n\n\n\n\n\nDependency Parsing\n\n\n\n\n\ndef exercise_dependency_parsing(text):\n    \"\"\"\n    EXERCISE 7: Dependency Parsing\n    ------------------------------\n    Analyze grammatical structure and relationships between words.\n\n    Critical for understanding: WHO sold WHAT to WHOM\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"STEP 7: DEPENDENCY PARSING\")\n    print(\"=\" * 80)\n\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    # Use a cleaner sample sentence\n    sample = \"Jean-Victor Pellerin sold the artwork to Wildenstein & Co. in 1955.\"\n    doc = nlp(sample)\n\n    print(f\"\\nğŸ“„ Sample Sentence: '{sample}'\")\n\n    print(\"\\nğŸ“Š Dependency Parse:\")\n    print(f\"\\n  {'Token':&lt;20} {'Dep':&lt;12} {'Head':&lt;20} {'Children'}\")\n    print(f\"  {'-'*20} {'-'*12} {'-'*20} {'-'*20}\")\n\n    for token in doc:\n        children = [child.text for child in token.children]\n        print(f\"  {token.text:&lt;20} {token.dep_:&lt;12} {token.head.text:&lt;20} {children}\")\n\n    # Visualize tree structure\n    print(\"\\nğŸŒ³ Dependency Tree Structure:\")\n    print(\"\"\"\n                            sold (ROOT)\n                             â”‚\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚           â”‚       â”‚       â”‚            â”‚\n    Jean-Victor   Pellerin  artwork  to          in\n    (compound)    (nsubj)   (dobj)  (prep)      (prep)\n                             â”‚       â”‚            â”‚\n                            the   Wildenstein   1955\n                           (det)    (pobj)      (pobj)\n                                     â”‚\n                                     &\n                                     â”‚\n                                    Co.\n    \"\"\")\n\n    # Extract subject-verb-object triples\n    print(\"\\nğŸ” Extracting Relationships:\")\n\n    for token in doc:\n        if token.dep_ == \"ROOT\" and token.pos_ == \"VERB\":\n            # Find subject\n            subjects = [child for child in token.children if \"subj\" in child.dep_]\n            # Find objects\n            objects = [child for child in token.children if \"obj\" in child.dep_]\n            # Find prepositional phrases\n            preps = [child for child in token.children if child.dep_ == \"prep\"]\n\n            print(f\"\\n  Verb: {token.text}\")\n            print(f\"  Subject(s): {[s.text for s in subjects]}\")\n            print(f\"  Object(s): {[o.text for o in objects]}\")\n\n            for prep in preps:\n                prep_obj = [child for child in prep.children if child.dep_ == \"pobj\"]\n                print(f\"  {prep.text.upper()}: {[p.text for p in prep_obj]}\")\n\n    return doc\n\ndep_doc = exercise_dependency_parsing(PROVENANCE_TEXT)\n\n\n\n\n\n\n\n\n\nPart of Speech (Pos) Tagging\n\n\n\n\n\ndef exercise_pos_tagging(text):\n    \"\"\"\n    EXERCISE 8: Part-of-Speech Tagging\n    ----------------------------------\n    Assign grammatical categories to each token.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"STEP 8: PART-OF-SPEECH (POS) TAGGING\")\n    print(\"=\" * 80)\n\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(text)\n\n    # Count POS tags\n    pos_counts = Counter([token.pos_ for token in doc])\n\n    print(\"\\nğŸ“Š POS Tag Distribution:\")\n    print(f\"\\n  {'POS Tag':&lt;10} {'Count':&lt;8} {'Examples'}\")\n    print(f\"  {'-'*10} {'-'*8} {'-'*40}\")\n\n    for pos, count in pos_counts.most_common():\n        examples = [t.text for t in doc if t.pos_ == pos][:5]\n        print(f\"  {pos:&lt;10} {count:&lt;8} {examples}\")\n\n    # Detailed view of first sentence\n    sample = \"Wildenstein & Co. sold to Stavros S. Niarchos in 1956.\"\n    sample_doc = nlp(sample)\n\n    print(f\"\\nğŸ“„ Detailed POS for: '{sample}'\")\n    print(f\"\\n  {'Token':&lt;15} {'POS':&lt;8} {'Tag':&lt;8} {'Description'}\")\n    print(f\"  {'-'*15} {'-'*8} {'-'*8} {'-'*30}\")\n\n    for token in sample_doc:\n        print(f\"  {token.text:&lt;15} {token.pos_:&lt;8} {token.tag_:&lt;8} {spacy.explain(token.tag_)}\")\n\n    # Find all verbs (important for provenance)\n    print(\"\\nğŸ” All Verbs in Text (transaction indicators):\")\n    verbs = [token.text for token in doc if token.pos_ == \"VERB\"]\n    print(f\"  {Counter(verbs).most_common()}\")\n\n    # Find all proper nouns (names, places)\n    print(\"\\nğŸ” All Proper Nouns (people, places, organizations):\")\n    proper_nouns = [token.text for token in doc if token.pos_ == \"PROPN\"]\n    print(f\"  {Counter(proper_nouns).most_common(20)}\")\n\n    return doc\n\npos_doc = exercise_pos_tagging(PROVENANCE_TEXT)\n\n\n\n\n\n\n\n\n\nNamed Entity Recognition (NER)\n\n\n\n\n\ndef exercise_named_entity_recognition(text):\n    \"\"\"\n    DOWNSTREAM TASK: Named Entity Recognition\n    -----------------------------------------\n    Identify and classify named entities (people, places, organizations, dates)\n\n    This is crucial for provenance research!\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"DOWNSTREAM TASK: NAMED ENTITY RECOGNITION (NER)\")\n    print(\"=\" * 80)\n\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(text)\n\n    # Extract entities\n    entities = [(ent.text, ent.label_, ent.start_char, ent.end_char)\n                for ent in doc.ents]\n\n    print(\"\\nğŸ“Š Named Entities Found:\")\n    print(f\"\\n  {'Entity':&lt;30} {'Type':&lt;12} {'Description'}\")\n    print(f\"  {'-'*30} {'-'*12} {'-'*30}\")\n\n    for ent_text, ent_label, _, _ in entities:\n        description = spacy.explain(ent_label)\n        print(f\"  {ent_text:&lt;30} {ent_label:&lt;12} {description}\")\n\n    # Group by entity type\n    print(\"\\nğŸ“Š Entities Grouped by Type:\")\n\n    entity_groups = {}\n    for ent in doc.ents:\n        if ent.label_ not in entity_groups:\n            entity_groups[ent.label_] = []\n        entity_groups[ent.label_].append(ent.text)\n\n    for label, ents in entity_groups.items():\n        print(f\"\\n  {label} ({spacy.explain(label)}):\")\n        unique_ents = list(set(ents))\n        for ent in unique_ents[:10]:\n            print(f\"    â€¢ {ent}\")\n\n    # Provenance-specific entity extraction\n    print(\"\\nğŸ” Provenance-Specific Extraction:\")\n    print(\"\\n  COLLECTORS/DEALERS:\")\n    persons = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n    orgs = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n    print(f\"    People: {list(set(persons))}\")\n    print(f\"    Organizations: {list(set(orgs))}\")\n\n    print(\"\\n  LOCATIONS:\")\n    locations = [ent.text for ent in doc.ents if ent.label_ in [\"GPE\", \"LOC\"]]\n    print(f\"    {list(set(locations))}\")\n\n    print(\"\\n  DATES:\")\n    dates = [ent.text for ent in doc.ents if ent.label_ == \"DATE\"]\n    print(f\"    {list(set(dates))}\")\n\n    return entities\n\nentities = exercise_named_entity_recognition(PROVENANCE_TEXT)\n\n\n\n\n\n\n\n\n\nInformation Extraction\n\n\n\n\n\ndef exercise_information_extraction(text):\n    \"\"\"\n    DOWNSTREAM TASK: Information Extraction\n    ---------------------------------------\n    Extract structured provenance events from unstructured text.\n\n    Goal: Create a timeline of ownership transfers\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"DOWNSTREAM TASK: INFORMATION EXTRACTION\")\n    print(\"=\" * 80)\n\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(text)\n\n    print(\"\\nğŸ” Extracting Provenance Events...\")\n    print(\"   Pattern: [SELLER] sold to [BUYER], [DATE]\\n\")\n\n    events = []\n\n    # Simple pattern matching for \"sold to\" phrases\n    for i, token in enumerate(doc):\n        if token.text.lower() == \"sold\" and i + 1 &lt; len(doc):\n            # Look for \"to\" after \"sold\"\n            if doc[i + 1].text.lower() == \"to\":\n                event = {\n                    'action': 'sold',\n                    'seller': None,\n                    'buyer': None,\n                    'date': None\n                }\n\n                # Find seller (look backwards for proper nouns/entities)\n                for j in range(i - 1, max(0, i - 10), -1):\n                    if doc[j].ent_type_ in [\"PERSON\", \"ORG\"]:\n                        # Get full entity\n                        for ent in doc.ents:\n                            if ent.start &lt;= j &lt; ent.end:\n                                event['seller'] = ent.text\n                                break\n                        break\n\n                # Find buyer (look forwards after \"to\")\n                for j in range(i + 2, min(len(doc), i + 15)):\n                    if doc[j].ent_type_ in [\"PERSON\", \"ORG\"]:\n                        for ent in doc.ents:\n                            if ent.start &lt;= j &lt; ent.end:\n                                event['buyer'] = ent.text\n                                break\n                        break\n\n                # Find date (look for DATE entity nearby)\n                for j in range(i, min(len(doc), i + 20)):\n                    if doc[j].ent_type_ == \"DATE\":\n                        for ent in doc.ents:\n                            if ent.start &lt;= j &lt; ent.end:\n                                event['date'] = ent.text\n                                break\n                        break\n\n                if event['seller'] or event['buyer']:\n                    events.append(event)\n\n    print(\"  ğŸ“‹ Extracted Provenance Events:\")\n    print(f\"\\n  {'#':&lt;4} {'Seller':&lt;25} {'Action':&lt;8} {'Buyer':&lt;25} {'Date'}\")\n    print(f\"  {'-'*4} {'-'*25} {'-'*8} {'-'*25} {'-'*10}\")\n\n    for i, event in enumerate(events, 1):\n        seller = event['seller'] or 'Unknown'\n        buyer = event['buyer'] or 'Unknown'\n        date = event['date'] or 'Unknown'\n        print(f\"  {i:&lt;4} {seller:&lt;25} {'sold':&lt;8} {buyer:&lt;25} {date}\")\n\n    # Timeline visualization\n    print(\"\\n  ğŸ“… Ownership Timeline:\")\n    print(\"  \" + \"=\" * 60)\n\n    for event in events:\n        if event['date']:\n            print(f\"  {event['date']}: {event['seller'] or '?'} â†’ {event['buyer'] or '?'}\")\n\n    return events\n\nevents = exercise_information_extraction(PROVENANCE_TEXT)\n\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\n\ndef print_summary():\n    \"\"\"Print a summary of all NLP pipeline steps.\"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"SUMMARY: NLP PIPELINE FOR PROVENANCE ANALYSIS\")\n    print(\"=\" * 80)\n\n    print(\"\"\"\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚                     NLP PIPELINE SUMMARY                        â”‚\n    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n    â”‚                                                                 â”‚\n    â”‚  Step 1: Sentence Segmentation                                  â”‚\n    â”‚          â””â”€â†’ Split text into sentences                          â”‚\n    â”‚                                                                 â”‚\n    â”‚  Step 2: Word Tokenization                                      â”‚\n    â”‚          â””â”€â†’ Split sentences into words/tokens                  â”‚\n    â”‚                                                                 â”‚\n    â”‚  Step 3: N-Gram Generation                                      â”‚\n    â”‚          â””â”€â†’ Create word sequences (bigrams, trigrams)          â”‚\n    â”‚                                                                 â”‚\n    â”‚  Step 4: Stemming/Lemmatization                                 â”‚\n    â”‚          â””â”€â†’ Reduce words to root forms                         â”‚\n    â”‚                                                                 â”‚\n    â”‚  Step 5: Stop Word Analysis                                     â”‚\n    â”‚          â””â”€â†’ Remove/identify low-value words                    â”‚\n    â”‚                                                                 â”‚\n    â”‚  Step 6: Word Embeddings                                        â”‚\n    â”‚          â””â”€â†’ Convert words to semantic vectors                  â”‚\n    â”‚                                                                 â”‚\n    â”‚  Step 7: Dependency Parsing                                     â”‚\n    â”‚          â””â”€â†’ Analyze grammatical relationships                  â”‚\n    â”‚                                                                 â”‚\n    â”‚  Step 8: POS Tagging                                            â”‚\n    â”‚          â””â”€â†’ Assign grammatical categories                      â”‚\n    â”‚                                                                 â”‚\n    â”‚  Downstream: NER + Information Extraction                       â”‚\n    â”‚          â””â”€â†’ Extract structured provenance events               â”‚\n    â”‚                                                                 â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n    ğŸ¯ KEY INSIGHTS FOR PROVENANCE TEXT:\n\n    â€¢ Multi-word entities (names, galleries) require n-gram analysis\n    â€¢ Transaction verbs (\"sold\", \"inherited\") indicate ownership changes\n    â€¢ Stop words like \"to\" and \"by\" carry meaning in this domain\n    â€¢ NER identifies people, organizations, places, and dates\n    â€¢ Dependency parsing reveals WHO sold WHAT to WHOM\n    \"\"\")\n\nprint_summary()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"END OF NLP PIPELINE EXERCISES\")\nprint(\"=\" * 80)\n\n\n\n\n\n\nResources\n\nIllustrated Guide to Transformer: A component-by-component breakdown analysis (2020).\nhttps://jinglescode.github.io/2020/05/27/illustrated-guide-transformer/\nThe Illustrated Word2vec.\nhttps://jalammar.github.io/illustrated-word2vec/\nText Analysis for Humanities Research (with a focus on Archaeology).\nhttps://github.com/alexbrandsen/Text-Analysis-for-Humanities-research?tab=readme-ov-file\n\n\nÂ© 2025 Coding Provenance Workshop â€” Licensed under a Creative Commons Attribution 4.0 International License.\nReuse is encouraged â€” we happily promote changes of custody. Just be sure to write a good provenance record and cite this workshop."
  },
  {
    "objectID": "dataset.html",
    "href": "dataset.html",
    "title": "Sample Provenance Datasets",
    "section": "",
    "text": "Knoedler Stock Books\n\nAbout the Dataset\nThis dataset contains 53,958 records transcribed from the 11 painting stock books and enhanced with information from the 21 paintings and watercolors sales books of M. Knoedler & Co.Â in New York (1872-1970).\nThe Getty Provenance Index\nKnoedler Dataset\nKnoedler Full Dataset\n\n\n\nDocumentation of Variables\n(click to expand each section)\n\n\n\n\n\n\nVariables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nType\nDescription\nNotes\nCIDOC-CRM / Linked Art Mapping\n\n\n\n\ntransaction_id\nString\nUnique identifier for the transaction event.\nGetty-derived identifier (URI suffix only).\nE7 Activity / la:ProvenanceEntry\n\n\ntransaction_activity_label\nString\nHuman-readable label describing the transaction activity.\nVerbatim label summarizing the transaction.\nE7 Activity â†’ rdfs:label\n\n\nacquisition_id\nString\nIdentifier of the acquisition sub-event within the transaction.\nGetty-derived identifier; represents method/context of acquisition.\nE8 Acquisition\n\n\nacquisition_label\nString\nLabel describing the acquisition type or method.\nCompound label (e.g.Â provenance + purchase).\nE8 Acquisition â†’ P2 has type\n\n\ntransaction_date\nDate / String\nDate associated with the transaction.\nRecorded transaction date; may include uncertainty (e.g.Â day = 00).\nE7 Activity â†’ P4 has time-span\n\n\nartwork_id\nString\nIdentifier of the artwork involved in the transaction.\nGetty-derived object identifier.\nE22 Human-Made Object\n\n\nartwork_label\nString\nTitle or short descriptive name of the artwork.\nVerbatim title as recorded.\nE22 Human-Made Object â†’ rdfs:label\n\n\nartwork_type\nString\nHigh-level type of the artwork.\nTypically Getty/AAT-derived textual label.\nE22 Human-Made Object â†’ P2 has type\n\n\nvisual_object_type_labels\nString\nMore specific representational or genre classification.\nTextual category (e.g.Â landscapes).\nE55 Type\n\n\nartist_id\nString\nIdentifier of the artist associated with the artwork.\nGetty-derived authority identifier.\nE21 Person\n\n\nartist_label\nString\nName of the artist associated with the artwork.\nVerbatim name; not normalized beyond source.\nE21 Person â†’ rdfs:label\n\n\nartists_nationality\nString\nNationality of the artist, when recorded.\nTextual value; not reconciled to authority files.\nE21 Person â†’ P107i is current or former member of\n\n\nbuyer_id\nString\nIdentifier of the buyer involved in the transaction.\nGetty-derived actor identifier.\nE39 Actor\n\n\nbuyer_label\nString\nName of the buyer involved in the transaction.\nVerbatim name only; locations removed.\nE39 Actor â†’ P22 transferred title to\n\n\nseller_id\nString\nIdentifier of the seller involved in the transaction.\nGetty-derived actor identifier.\nE39 Actor\n\n\nseller_label\nString\nName of the seller involved in the transaction.\nVerbatim name only; locations removed.\nE39 Actor â†’ P23 transferred title from\n\n\npayment_quantity\nNumeric\nMonetary amount recorded for the transaction.\nNumeric value without currency symbol.\nE97 Monetary Amount\n\n\npayment_currency\nString\nCurrency in which the payment was made.\nTextual currency label (not ISO-normalized).\nE98 Currency\n\n\n\n\n\n\n\n\n\n\nArt Institute of Chicago\n\nAbout the Dataset\nThis dataset compiles provenance records from the Art Institute of Chicagoâ€™s online collection database.\nIt includes artworks such as paintings, sculptures, prints, drawings, and decorative arts.\nIn the original API, provenance information appears as free text. For the purposes of this workshop, this material has been transformed into structured provenance events, where each event includes one or more involved parties (people, families, institutions) and other semantic components. The information of all provenance statements remains exactly as provided by the Art Institute of Chicago â€” no corrections, additions, or interpretative changes have been introduced.\nAPI â€” Art Institute of Chicago\nArt Institute of Chicago Provenance Dataset\n\n\n\nDocumentation of Variables\n(click to expand each section)\n\n\n\n\n\n\nArtwork-Level Variables\n\n\n\n\n\nThese fields sit at the top level of each JSON object.\n\n\n\n\n\n\n\n\nVariable\nType\nDescription\n\n\n\n\nid\nInteger\nInternal numeric identifier for each artwork (unique).\n\n\ntitle\nString\nPublic title of the work.\n\n\nalt_titles\nString or array, nullable\nAlternative titles when available.\n\n\nmain_reference_number\nString\nAccession/inventory number.\n\n\ndate_start\nInteger\nEarliest possible creation year (BCE negative, CE positive).\n\n\ndate_end\nInteger\nLatest possible creation year.\n\n\nartist_display\nString\nDisplay text describing maker/culture/origin.\n\n\nplace_of_origin\nString\nNormalized place name extracted from the record.\n\n\ndimensions\nString\nFree-text measurements.\n\n\nmedium_display\nString\nMaterials and techniques.\n\n\ninscriptions\nString, nullable\nDescription or transcription of inscriptions.\n\n\ncredit_line\nString\nHow the work entered the collection (gift, purchase, etc.).\n\n\nprovenance_text\nString\nNarrative provenance as written by AIC curators.\n\n\nevents_structured\nArray\nParsed provenance events derived from the narrative.\n\n\n\n\n\n\n\n\n\n\n\n\nProvenance Event-Level Variables\n\n\n\n\n\nEach entry in events_structured represents a structured provenance fragment.\n\n\n\n\n\n\n\n\nVariable\nType\nDescription\n\n\n\n\nevent\nString\nEvent identifier (e.g., â€œevent_1â€). Groups components belonging to the same ownership step.\n\n\nvalue\nString\nExtracted fragment referring to a person, method, date, place, etc.\n\n\ntype\nArray of strings\nSemantic labels defining the nature of value. May include:\n\n\n\n\nâ€¢ Person â€” Named individual\n\n\n\n\nâ€¢ Party â€” Any owner (person, family, institution)\n\n\n\n\nâ€¢ Group â€” Collective/institutional actor\n\n\n\n\nâ€¢ Receiver â€” Party acquiring the work\n\n\n\n\nâ€¢ Method â€” How the object changed hands (â€œsoldâ€, â€œgivenâ€)\n\n\n\n\nâ€¢ Time â€” Specific dates or years\n\n\n\n\nâ€¢ Vagueness â€” Uncertain expressions (â€œby 1938â€)\n\n\n\n\nâ€¢ Name â€” Explicit name tag (overlaps with Person/Party)\n\n\n\n\n\n\n\nÂ© 2025 Coding Provenance Workshop â€” Licensed under a Creative Commons Attribution 4.0 International License.\nReuse is encouraged â€” we happily promote changes of custody. Just be sure to write a good provenance record and cite this workshop."
  },
  {
    "objectID": "dataanalysis.html",
    "href": "dataanalysis.html",
    "title": "Data Preprocessing, Analysis, and a New World of Research Questions",
    "section": "",
    "text": "Schema\n\nI have messy data, now what? â€“ Data Preprocessing\n\nLetâ€™s Get to the Analysis\n\nBasics of Descriptive Statistics\n\n\nData Visualization Principles and Exploratory Plots\n\nMost Importantly: What about your Research Questions?\n\n\n\nContent Summary\nFrom Mess to Meaning: A Beginnerâ€™s Guide to Cleaning and Analyzing Your Data\n\n\nSlides\n\n\n\n\n\n\nResources\n\nTools\n\nDataviz Project\niAnalyzer\nOpenRefine\nProgramming Historian\nPython Graph Gallery\nRAWGraphs\nVoyant Tools\n\n\n\nSelected Bibliography\n\nKarsdorp, Folgert. Python Programming for the Humanities.\nhttps://www.karsdorp.io/python-course/\nKarsdorp, Folgert, Mike Kestemont, and Allen Riddell. Humanities Data Analysis: Case Studies with Python. Princeton University Press, 2021.\nhttps://www.humanitiesdataanalysis.org/\nMariani, Fabio, Max Koss, and Lynn Rother. â€œPeople Information in Provenance Data: Biographical Entity Linking with Wikidata and ULAN.â€ Å½ivot Umjetnosti 114, no. 1 (2024): 148â€“161.\nhttps://doi.org/10.31664/zu.2024.114.07\nâ€œCreating a Data Cleaning Workflow.â€ https://cghlewis.com/blog/data_clean_02/\n\n\n\n\n\nNetwork Analysis\n\nSchema\n\nA Networked World\nWhat Is a Network\nNetwork Syntax and Ontology\nVisualizing Is not Enough: Centrality Measures & Community Detection\nIntro to Gephi\n\n\n\nContent Summary\nAn Introduction to Network Analysis: Seeing the World in Connections\n\n\nSlides\n\n\n\n\n\n\nActivity\nDownload newtork Art Institute of Chicago. Connections are made through objects (if two parties owned the same object they are connected). Attributes: objects Edges Nodes\n\n\nResources\n\nTools\n\nCosmograph\nGephi\n\n\n\nSelected Bibliography\n\nAhnert, Ruth, and Sebastian E. Ahnert. Tudor Networks of Power. Oxford University Press, 2023.\nhttps://doi.org/10.1093/oso/9780198858973.001.0001\nAhnert, Ruth et al. The Network Turn. Cambridge University Press, 2020.\nhttps://doi.org/10.1017/9781108866804\nBarabÃ¡si, Albert-LÃ¡szlÃ³, et al. Network Science.\nhttp://networksciencebook.com/\nFraiberger, Samuel P., Roberta Sinatra, Markus Resch, Christoph Riedl, and Albert-LÃ¡szlÃ³ BarabÃ¡si. â€œQuantifying Reputation and Success in Art.â€ Science 362, no. 6416 (2018): 825â€“829.\nhttps://doi.org/10.1126/science.aau7224\nHuemer, Christian. â€œThe Provenance of Provenances.â€ In Collecting & Provenance: A Multidisciplinary Approach, edited by Jane C. Milosch and Nick Pearce. Rowman & Littlefield, 2019.\nLincoln, Matthew D. â€œTangled Metaphors: Network Thinking and Network Analysis in the History of Art.â€ In The Routledge Companion to Digital Humanities and Art History, Routledge, 2020. https://doi-org.proxy1.lib.uwo.ca/10.4324/9780429505188\nMcNeill, J. R., and William H. McNeill. The Human Web: A Birdâ€™s-Eye View of World History. W. W. Norton, 2003.\nPorras, Stephanie. â€œKeeping Our Eyes Open: Visualizing Networks and Art History.â€ Artl@s Bulletin 6, no. 3 (2017).\nhttps://docs.lib.purdue.edu/artlas/vol6/iss3/3\nSchich, Maximilian, Christian Huemer, Patryk Adamczyk, Lev Manovich, and Yang-Yu Liu. â€œNetwork Dimensions in the Getty Provenance Index.â€ arXiv (2017). https://arxiv.org/abs/1706.02804\n\n\nÂ© 2025 Coding Provenance Workshop â€” Licensed under a Creative Commons Attribution 4.0 International License.\nReuse is encouraged â€” we happily promote changes of custody. Just be sure to write a good provenance record and cite this workshop."
  },
  {
    "objectID": "datamodels.html",
    "href": "datamodels.html",
    "title": "Knowledge Design. Data Models, Ontologies & Controlled Vocabularies",
    "section": "",
    "text": "Schema\n\nWhat is a Dataset?\nRecipient of Knowledge: Relational Databases / Graph Databases\n\nEpistemological Questions: Data Models & Ontologies\n\nCrafting Data\n\nConnecting our Data to the World: Controlled Vocabularies\n\nData Formats: CSV, JSON, Linked Open Data\n\nShare it!\n\nCurrent Challenges: Collection Management Systems\n\n\n\nContent Summary\nThe Secret Life of Data: From Real-World Events to Digital Insights\n\n\nSlides\n\n\n\n\n\n\nResources\n\nCARE Principles for Indigenous Data Governance â€” Global Indigenous Data Alliance\nCIDOC-CRM, Version 7.3 â€” CIDOC CRM Special Interest Group\nGetty Vocabularies (AAT, ULAN, TGN) â€” Getty Research Institute\nLinked Art â€” Linked Art Community\nThe FAIR Guiding Principles for Scientific Data Management and Stewardship â€” Wilkinson et al.\nThe Virtual International Authority File (VIAF)\nWikidata â€” A Free and Open Knowledge Base\n\n\nSelected Bibliography\n\nBishop, Claire. â€œAgainst Digital Art History.â€ International Journal for Digital Art History 3 (2018).\nhttps://doi.org/10.11588/dah.2018.3.49915\nDâ€™Ignazio, Catherine, and Lauren F. Klein. Data Feminism. MIT Press, 2023.\nhttps://data-feminism.mitpress.mit.edu/\nDrucker, J. â€œIs There a â€œDigitalâ€ Art History?â€ Visual Resources, 29(1â€“2) (2013): 5â€“13. https://doi.org/10.1080/01973762.2013.761106\nLove, Heather. â€œClose but Not Deep: Literary Ethics and the Descriptive Turn.â€ New Literary History 41, no. 2 (2010): 371â€“391.\nhttp://www.jstor.org/stable/40983827\nPoirier, Lindsay. â€œEthnographies of Datasets: Teaching Critical Data Analysis Through R Notebooks.â€ 2020.\nhttps://lindsaypoirier.github.io/publication/2020-12-01-Ethnographies-of-Datasets-Teaching-Critical-Data-Analysis-through-R-Notebooks\nRicaurte, Paola. â€œData Epistemologies, the Coloniality of Power, and Resistance.â€ Television & New Media 20, no. 4 (2019): 350â€“365.\nhttps://doi.org/10.1177/1527476419831640\nRother, Lynn. â€œUncanny Provenance: Art History and Its Double / Unheimliche Provenienz: Kunstgeschichte und ihr DoppelgÃ¤nger.â€ Texte zur Kunst 32, no. 128 (2022): 84â€“97.\nRother, Lynn. â€œInstitutionalisierte Institutionskritik: Lynn Rother Ã¼ber â€˜Wege der Kunstâ€™ im Museum Rietberg, ZÃ¼rich, sowie â€˜Zerrissene Moderneâ€™ und â€˜Der Sammler Curt Glaserâ€™ im Kunstmuseum Basel.â€ Texte zur Kunst 33, no. 131 (2023): 214â€“219.\nRother, Lynn, Max Koss, and Fabio Mariani. â€œTaking Care of History: Toward a Politics of Provenance Linked Open Data in Museums.â€ In Perspectives on Data, edited by Emily Lew Fry and Erin Canning. Art Institute of Chicago, 2022.\nhttps://www.artic.edu/digital-publications/37/perspectives-on-data/25/taking-care-of-history\n\n\nÂ© 2025 Coding Provenance Workshop â€” Licensed under a Creative Commons Attribution 4.0 International License.\nReuse is encouraged â€” we happily promote changes of custody. Just be sure to write a good provenance record and cite this workshop."
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "Learning How to Code",
    "section": "",
    "text": "Schema\n\nWhat Is Python?\nUnderstanding Python as a Language\nWhy Generative AI Is Helpful for Coding Purposes\nWriting Good Prompts\nGet Familiar with the Environments â€“ Google Colab\n\n\n\nSlides\n\n\n\n\n\n\nActivity\n\nPython Scripts\n\n\n\n\n\n\nOpen CSV\n\n\n\n\n\nimport pandas as pd\n\nfile_path = \"/content/knoedler.csv\"\n\ntry:\n    df = pd.read_csv(file_path, low_memory=False)\n    print(\"âœ“ CSV file loaded successfully!\\n\")\nexcept FileNotFoundError:\n    print(f\"Error: File '{file_path}' not found.\")\n    print(\"Download from: https://github.com/thegetty/provenance-index-csv/tree/main/knoedler\")\n    exit()\n\n\n\n\n\n\n\n\n\nRead column names\n\n\n\n\n\nprint(\"=\" * 60)\nprint(\"COLUMN NAMES\")\nprint(\"=\" * 60)\ncolumns = df.columns.tolist()\nfor i, col in enumerate(columns, 1):\n    print(f\"  {i}. {col}\")\nprint(f\"\\nTotal columns: {len(columns)}\\n\")\n\n\n\n\n\n\n\n\n\nCheck number of rows\n\n\n\n\n\nprint(\"=\" * 60)\nprint(\"DATASET DIMENSIONS\")\nprint(\"=\" * 60)\nnum_rows = len(df)\nnum_cols = len(df.columns)\nprint(f\"  Number of rows: {num_rows:,}\")\nprint(f\"  Number of columns: {num_cols}\")\nprint(f\"  Total cells: {num_rows * num_cols:,}\\n\")\n\n\n\n\n\n\n\n\n\nUnique values in a column\n\n\n\n\n\nprint(\"=\" * 60)\nprint(\"UNIQUE VALUES ANALYSIS\")\nprint(\"=\" * 60)\n\n# Function to get unique values for any column\ndef get_unique_values(dataframe, column_name):\n    \"\"\"Return unique values and count for a specified column.\"\"\"\n    if column_name not in dataframe.columns:\n        print(f\"Column '{column_name}' not found in dataset.\")\n        return None\n\n    unique_vals = dataframe[column_name].dropna().unique()\n    unique_count = len(unique_vals)\n\n    print(f\"\\nColumn: '{column_name}'\")\n    print(f\"  Unique values: {unique_count:,}\")\n    print(f\"  Missing values: {dataframe[column_name].isna().sum():,}\")\n\n    # Show sample of unique values (first 10)\n    if unique_count &gt; 0:\n        print(f\"  Sample values (up to 10):\")\n        for val in list(unique_vals)[:10]:\n            print(f\"    - {val}\")\n\n    return unique_vals\n\n# Analyze a few columns (adjust column names based on actual data)\n# Common columns in Knoedler data: 'artist_name', 'title', 'genre', 'sale_date'\nsample_columns = columns[:3]  # Analyze first 3 columns as example\n\nfor col in sample_columns:\n    get_unique_values(df, col)\n\n\n\n\n\n\n\n\n\nSave a CSV (filtered or processed data)\n\n\n\n\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"SAVING PROCESSED DATA\")\nprint(\"=\" * 60)\n\n# Example: Save a summary of missing data per column\nmissing_data = pd.DataFrame({\n    'column': df.columns,\n    'missing_count': df.isnull().sum().values,\n    'missing_percent': (df.isnull().sum().values / len(df) * 100).round(2),\n    'unique_values': [df[col].nunique() for col in df.columns]\n})\n\noutput_file = \"provenance_data_summary.csv\"\nmissing_data.to_csv(output_file, index=False)\nprint(f\"âœ“ Summary saved to '{output_file}'\\n\")\n\n# Optional: Save filtered subset\n# Example: Save rows where a specific column has missing data\n# df_with_gaps = df[df['artist_name'].isna()]\n# df_with_gaps.to_csv(\"records_missing_artist.csv\", index=False)\n\nprint(\"=\" * 60)\nprint(\"SCRIPT COMPLETE\")\nprint(\"=\" * 60)\n\n\n\n\n\n\nResources\n\nGoogle Colab\nProgramming Historian\nPython Documentation\nPython Official Website\n\n\nÂ© 2025 Coding Provenance Workshop â€” Licensed under a Creative Commons Attribution 4.0 International License.\nReuse is encouraged â€” we happily promote changes of custody. Just be sure to write a good provenance record and cite this workshop."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Coding Provenance Workshop. First Edition",
    "section": "",
    "text": "This workshop explores the intersection between provenance studies and computational techniques, bringing together a selected cohort of scholars, curators, and practitioners to explore digital approaches, data modeling, computational analysis, and data visualization.\n\nFrom 18 to 21 December 2025 â€” Leuphana University of LÃ¼neburg\nRoom C.40.606, Seminarraum, Main Building at Leuphana University of LÃ¼neburg\n\n\nSessions\n\nKnowledge Design. Data Models, Ontologies & Controlled Vocabularies\nLearning How to Code\nFrom Raw Text to Structured Data\nData Preprocessing, Analysis, and a New World of Research Questions\n\nNetwork Analysis\n\n\nData Visualization\nBibliography & other Resources\n\n\n\nDecember Schedule\n\n\n\n\nDay 1 Â· Thursday 18 December\n\n\n\n\nTime\nSession\n\n\n\n\n15 : 15\nParticipantâ€™s Registration\n\n\n15 : 30 â€“ 16 : 00\nWelcome Remarks by the President of Leuphana University of LÃ¼neburg\n\n\n16 : 00 â€“ 17 : 00\nOpening Panel\n\n\n17 : 00 â€“ 18 : 00\nFood & Drinks\n\n\n18 : 30 \nMuseum LÃ¼neburg Visit & Little Reception\n\n\n\n\n\n\nDay 2 Â· Friday 19 December\n\n\n\n\nTime\nSession\n\n\n\n\n09 : 00 â€“ 12 : 00\nKnowledge Design\n\n\n12 : 00 â€“ 13 : 00\nLearning How to Code\n\n\n13 : 00 â€“ 14 : 00\nLunch Break - Klippo\n\n\n14 : 15 â€“ 16 : 45\nText to Tabular Data\n\n\n17 : 30 \nGerman Salt Museum Visit\n\n\n20 : 15 \nDinner - FrappÃ©\n\n\n\n\n\n\nDay 3 Â· Saturday 20 December\n\n\n\n\nTime\nSession\n\n\n\n\n09 : 00 â€“ 13 : 00\nData Analysis\n\n\n13 : 00 â€“ 14 : 00\nLunch Break - Klippo\n\n\n14 : 15 â€“ 16 : 00\nNetwork Analysis\n\n\n16 : 00 â€“ 17 : 30\nInformation Visualization\n\n\n18 : 30 \nChristmas Market\n\n\n20 : 15 \nDinner - Zweilieben Pizzeria\n\n\n\n\n\n\nDay 4 Â· Sunday 21 December\n\n\n\n\nTime\nSession\n\n\n\n\n10 : 00\nClosing Panel of Participants\n\n\n13 : 00\nEnd of Workshop\n\n\n\n\n\n\nÂ© 2025 Coding Provenance Workshop â€” Licensed under a Creative Commons Attribution 4.0 International License.\nReuse is encouraged â€” we happily promote changes of custody. Just be sure to write a good provenance record and cite this workshop."
  },
  {
    "objectID": "datavisualization.html",
    "href": "datavisualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Schema\n\nA very Brief History of Visualizations\n\nVisualization Elements\n\nWhat Do We Want to Communicate?\n\n\n\nSlides\n\n\n\n\n\n\nResources\n\nVisualization Examples\n\nCÃ¤sar Flaischlenâ€™s Graphische Litteratur-Tafel\nRoyal Constellations â€” VisualCinnamon (Nadieh Bremer)\nMuseo del Prado â€” Portrait Visualization (Elizabeth of Bourbon & Mariana of Austria)\nDigital Curator â€” Computational Art History Tools\nVisualCinnamon â€” New York Times Digital Trackers\nTudor Networks of Power â€” Interactive Network Project\nGiorgia Lupi â€” â€œData Items: A Fashion Landscapeâ€ (MoMA)\n\n\n\nSelected Bibliography\n\nCairo, Alberto. How Charts Lie: Getting Smarter About Visual Information. W. W. Norton, 2020.\nDrucker, Johanna. Graphesis: Visual Forms of Knowledge Production. Harvard University Press, 2014.\nMeirelles, Isabel. Design for Information. Rockport Publishers, 2013.\nSedig, Kamran, and Paul Parsons. Design of Visualizations for Humanâ€“Information Interaction: A Pattern-Based Framework. Springer Nature, 2016.\nhttps://doi.org/10.1007/978-3-031-02602-7\nTuscher, Michaela, et al.Â  â€œNodes, Edges, and Artistic Wedges: A Survey on Network Visualization in Art History.â€ Computer Graphics Forum (2025): e70154.\nhttps://doi.org/10.1111/cgf.70154\n\n\nÂ© 2025 Coding Provenance Workshop â€” Licensed under a Creative Commons Attribution 4.0 International License.\nReuse is encouraged â€” we happily promote changes of custody. Just be sure to write a good provenance record and cite this workshop."
  }
]